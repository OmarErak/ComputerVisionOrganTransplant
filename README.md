# Organ Transplant Decision Based on Liver Steatosis Classification

This repository contains the code for our project on **automated liver steatosis assessment** from Whole Slide Images (WSIs) using pathology foundation models.  

The pipeline combines:

- **TRIDENT** for scalable WSI processing and patch-level feature extraction (CONCH v1.5). 
- **TITAN** for slide-level multimodal feature analysis, zero-shot classification, and slide embeddings. 

We predict whether each liver WSI has **High steatosis (>30%)** or **Low steatosis (<30%)**, and compare:

- **Zero-shot TITAN classification** (no training labels)  
- **Supervised MLP classifier** trained on frozen TITAN slide embeddings  

---

## Repository Structure

```text
.
‚îú‚îÄ‚îÄ download_organize_wsi.py       # Download + filter GTEx WSIs + create Total_WSI.csv
‚îú‚îÄ‚îÄ run_trident_feature_extraction.py   # TRIDENT: WSI ‚Üí CONCH v1.5 patch features
‚îú‚îÄ‚îÄ zeroshot.py                    # TITAN zero-shot steatosis classification
‚îú‚îÄ‚îÄ supervised.py                  # Supervised MLP classifier on TITAN embeddings
‚îú‚îÄ‚îÄ GTEx Portal.csv                # Raw pathology notes downloaded from GTEx
‚îú‚îÄ‚îÄ Total_WSI.csv                  # Cleaned dataset (labels + WSI metadata)
‚îú‚îÄ‚îÄ requirements.txt               # All Python dependencies
‚îî‚îÄ‚îÄ README.md

````

---

## Data

### Total_WSI.csv

`Total_WSI.csv` contains slide-level metadata used across scripts. At minimum it is expected to include:

* `Tissue Sample ID` ‚Äì unique identifier for each WSI
* `Label` ‚Äì steatosis label (e.g. `0 = High`, `1 = Low` in the code)
* Optional columns ‚Äì e.g. free-text **pathology notes** or reports used for manual labeling

Your WSIs themselves are **not** stored in this repository; you should keep them in your own storage and point `feature_extraction.py` to that directory.

Features used in this project can be found at: https://drive.google.com/drive/folders/1OSpqz3AhOmgKejD7Klx-XLPHsLc0E5Up
---
---
### Download & Organize GTEx WSIs
The script `download_organize_wsi.py` automatically:

* Loads the GTEx Portal.csv metadata
* Filters slides with clear steatosis/fibrosis percentage in the pathology notes
* Downloads the corresponding .svs whole-slide images

#### Run the script
```
python download_organize_wsi.py
```
---
## Dependencies

* Python ‚â• 3.9
* PyTorch ‚â• 2.0
* `transformers`
* `huggingface_hub`
* `h5py`, `pandas`, `numpy`, `scikit-learn`
* `tqdm`, `matplotlib` (optional, for plots)
* **TRIDENT** (installed from source) ([GitHub][1])
* **TITAN** (installed from source + Hugging Face access) ([GitHub][2])

> ‚ö†Ô∏è **Medical use disclaimer:** TRIDENT, CONCH, and TITAN are released for **non-commercial research only** under their respective licenses. This project is a research prototype and **not** a clinical decision support tool.

---

## 1. Environment Setup

### 1.1 Clone this repository

```bash
git OmarErak/ComputerVisionOrganTransplant.git
cd OmarErak/ComputerVisionOrganTransplant
```

### 1.2 Install a base environment

You can adapt this to your preferred environment manager:

```bash
conda create -n liver-titan python=3.9 -y
conda activate liver-titan

pip install --upgrade pip
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121  # or CPU wheels
pip install transformers huggingface_hub h5py pandas numpy scikit-learn tqdm matplotlib
```

---

## 2. Install TRIDENT

TRIDENT is a toolkit from MahmoodLab for WSI preprocessing and feature extraction. ([GitHub][1])

```bash
# From somewhere outside this repo:
git clone https://github.com/mahmoodlab/TRIDENT.git
cd TRIDENT
pip install -e .
cd ..
```

TRIDENT provides:

* Tissue segmentation
* Patch extraction
* Patch feature extraction using **CONCH v1.5** and other foundation models
* Slide-level feature extraction utilities

---

## 3. Install TITAN

TITAN is a multimodal whole-slide foundation model trained on CONCH v1.5 patch features.

```bash
git clone https://github.com/mahmoodlab/TITAN.git
cd TITAN
pip install -e .
cd ..
```

### 3.1 Request access to TITAN and CONCH v1.5 weights

Both TITAN and CONCH v1.5 are hosted on Hugging Face and require **gated access**:

* TITAN model card: `https://huggingface.co/MahmoodLab/TITAN` 
* CONCH v1.5 model card: `https://huggingface.co/MahmoodLab/conchv1_5` 

Follow the instructions on each model page to **request access with an institutional email**.

### 3.2 Login to Hugging Face

```python
from huggingface_hub import login

login()  # paste your HF token from https://huggingface.co/settings/tokens
```
 
---

## 4. Step 1 ‚Äì Patch Feature Extraction (`run_trident_feature_extraction.py`)

This script uses **TRIDENT** to:

1. Read WSIs from a directory
2. Perform tissue detection / patch extraction
3. Extract **CONCH v1.5** patch embeddings (512√ó512 at 20√ó)
4. Save features and coordinates to HDF5 (`.h5`) files compatible with TITAN ([GitHub][2])

### Before running the script:

* Request access to CONCH v1.5:
```
üîó https://huggingface.co/MahmoodLab/conchv1_5
```

* Generate a token:
```
üîó https://huggingface.co/settings/tokens
```

* Log in via the script automatically by passing:
```
--hf_token "YOUR_TOKEN"
```

**Example running script:**

```bash
python run_trident_feature_extraction.py \
    --trident_dir "./TRIDENT" \
    --wsi_dir "./" \
    --job_dir "./features" \
    --hf_token "YOUR_HF_TOKEN"
```

Typical output structure:

```text
/path/to/features/
‚îú‚îÄ‚îÄ <SLIDE_ID_1>.h5
‚îú‚îÄ‚îÄ <SLIDE_ID_2>.h5
‚îî‚îÄ‚îÄ ...
```

Each `.h5` file should contain:

* `features` ‚Äì [num_patches, 768] CONCH patch embeddings
* `coords`   ‚Äì [num_patches, 2] patch coordinates
* `coords.attrs["patch_size_level0"]` ‚Äì distance between adjacent patches at level 0

---

### 5. Step 2 ‚Äì Zero-Shot Classification (`zeroshot.py`)

`zeroshot.py` is a **self-contained script** that uses TITAN to run zero-shot steatosis classification on all slides listed in `Total_WSI.csv`.

At the top of the file there is a **configuration block**:

```python
# --- CONFIGURATION ---
CSV_PATH = 'Total_WSI.csv'
FEATURE_DIR = '../features-20251122T053023Z-1-001/features/20x_512px_0px_overlap/features_conch_v15'
OUTPUT_CSV = 'titan_steatosis_predictions.csv'

# Label mapping and class names
# Label 0 = High Steatosis (>30%)
# Label 1 = Low Steatosis (<30%)
CLASS_NAMES = ["High Steatosis (>30%)", "Low Steatosis (<30%)"]

# Text prompts for zero-shot classification (prompt ensemble)
STEATOSIS_PROMPTS = {
    "High Steatosis (>30%)": [
        "A histology slide dominated by large white fat vacuoles.",
        "Severe fatty change with crowded clear circular spaces.",
        "Macrovesicular steatosis replacing more than half the tissue.",
        "Pathology image showing abundant lipid droplets displacing nuclei.",
        "Swiss cheese texture with very little solid pink tissue."
    ],
    "Low Steatosis (<30%)": [
        "A histology slide dominated by solid pink cytoplasm.",
        "Preserved hepatic architecture with no significant fat vacuoles.",
        "Dense eosinophilic tissue without many white holes.",
        "Intact parenchyma consisting of uniform hepatocytes.",
        "Tissue showing mostly nuclei and cytoplasm, not fat."
    ]
}
```

To run zero-shot inference you only need to:

1. Edit `CSV_PATH`, `FEATURE_DIR`, and `OUTPUT_CSV` to match your setup.
2. Optionally edit `STEATOSIS_PROMPTS` if you want to experiment with different prompts.
3. Run:

```bash
python zeroshot.py
```

What the script does:

* Loads TITAN from Hugging Face.
* Builds a zero-shot classifier from the prompt ensemble.
* For each slide ID in `Total_WSI.csv`:

  * Loads the corresponding `.h5` feature file from `FEATURE_DIR`.
  * Computes the TITAN slide embedding.
  * Applies zero-shot classification (High vs Low steatosis).
* Prints:

  * Accuracy, precision, recall, F1-score.
  * Confusion matrix counts.
* Saves:

  * A **PNG confusion matrix** (`zeroshot_confusion_matrix.png`).
  * A detailed **per-slide CSV** (`OUTPUT_CSV`) containing:

    * `Tissue Sample ID`
    * `True Label`
    * `Predicted Label`
    * `Predicted Class`
    * `Score_High`, `Score_Low` (raw scores / logits per class).

---

### 6. Step 3 ‚Äì Supervised Classification (`supervised.py`)

`supervised.py` has a similar style: configuration is done via **global variables at the top of the file**, and then you simply run the script.

A typical configuration block looks like:


To run supervised training:

1. Edit `CSV_PATH`, `FEATURE_DIR`, and `OUTPUT_DIR` to match your environment.
2. Optionally adjust training hyperparameters (learning rate, dropout, batch size, etc.).
3. Run:

```bash
python supervised.py
```

What the script does:

* Loads TITAN and computes (or loads) slide embeddings from the `.h5` feature files.
* Joins embeddings with labels from `Total_WSI.csv`.
* Trains an MLP classifier on frozen TITAN embeddings using **stratified K-fold cross-validation** (e.g. 5 folds).
* Optionally runs Bayesian optimization inside the script to refine LR / dropout / weight decay.
* Outputs:

  * Console metrics per fold and the mean accuracy / precision / recall / F1.
  * Confusion matrices and training curves (loss/accuracy plots) saved under `OUTPUT_DIR`.
  * A CSV with per-fold metrics and, optionally, a saved model checkpoint (e.g. `best_model.pt`).

---

## 7. Reproducibility

To make your experiments reproducible:

* Fix random seeds inside `zeroshot.py` and `supervised.py` (Python, NumPy, PyTorch).
* Document which version of TRIDENT, TITAN, and PyTorch you used.
* Save:

  * The exact prompts used for zero-shot classification
  * The final hyperparameters for the supervised MLP

---

## 8. References

If you use this repository in academic work, please cite:

* **TRIDENT** ‚Äì scalable WSI processing toolkit. 
* **CONCH v1.5** ‚Äì vision‚Äìlanguage pathology foundation model. 
* **TITAN** ‚Äì multimodal whole-slide foundation model. 


---

## 9. Contact

For questions about this code:

TBD

For questions about TRIDENT, CONCH, or TITAN, please refer to the official MahmoodLab repositories and documentation.
